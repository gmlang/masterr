I"]<p>We all know we need to pre-process data before we build models. However, if you’re building a XGboost model, you can avoid many of the pre-processing steps. I’ve done some google search and book reading, and the following list summarizes my findings.</p>

<ul>
  <li>Remove zero or near-zero variance predictors. (Don’t do it for xgboost or any tree based methods)</li>
  <li>Remove highly correlated predictors. (Not needed for xgboost or other tree based ensemble methods)</li>
  <li>BoxCox, Yeo-Johnson, exponential transformation of Manly (1976) and other type of transformations of the same spirit on the predictors. (Not needed for xgboost)</li>
  <li>Centering and scaling the predictors. (Not needed for xgboost)</li>
  <li>Missing data imputation. (Not needed for xgboost as it supports missing internally by default)</li>
  <li>One-hot encoding or dummy variable creation of categorical predictors (Needed for xgboost)</li>
  <li>PCA (May help with performance for xgboost)</li>
</ul>

<p>The phrase “not needed” means, by and large, doing it won’t improve (or worsen) model performance.</p>
:ET