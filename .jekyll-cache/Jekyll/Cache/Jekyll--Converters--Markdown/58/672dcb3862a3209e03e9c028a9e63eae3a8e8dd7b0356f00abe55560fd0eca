I"Û<p>When it comes to model interpretability and prediction performance, we often just canâ€™t have the best of both worlds. But we almost always want a model that both makes sense and predicts well. This is especially true if we  have expert domain (for example, medicine) knowlege. When presented with a model, the domain expertâ€™s state of mind is usually like this:</p>

<ul>
  <li>I know more X causes more Y (by my domain knowlege, experience, intuition and etc.), the model has a big positive coefficient in front of X, so it makes sense. I trust the model.</li>
  <li>I never thought more X could cause more Y, but the model says so. This is interesting. Why would this be?</li>
  <li>Does the model say more X cause more Y? What? We donâ€™t know the answer because the model is a black box? Well, I donâ€™t know if I can trust a black box. Wait, the validation results are really good and this implies the model predicts really well. Well, I still donâ€™t know if I should trust a black box.</li>
</ul>

<p>Thereâ€™s no general solution to resolve the tension between predictibility and interpretability. Itâ€™s all situational. In practice, if the goal is prediction, most of the time, I have no problem sacrificing interpretability by using a black box as long as it validates well on a new unseen dataset. Iâ€™d use the black box in production, see how well it predicts and tweak it when its performance gets worse. After all, model building and fine tuning is a continuous process.</p>

:ET